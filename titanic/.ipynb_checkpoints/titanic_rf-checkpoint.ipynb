{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Engineering**\n",
    "==========\n",
    "모델에 직접 적용하기 전에 데이터들을 분석하고 이를 training하기 좋게 변환하는 과정이 중요하다 생각하여 하는 과정임 - Sina의 Kernel 참조하였음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re as re\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('C:/Users/sangkyu/git/kaggle/titanic/input/train.csv')\n",
    "test = pd.read_csv('C:/Users/sangkyu/git/kaggle/titanic/input/test.csv')\n",
    "full_data = [train,test]\n",
    "print (train.info())\n",
    "PassengerId = test['PassengerId']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['Pclass','Survived']].groupby(['Pclass'],as_index=False).mean()\n",
    "# as_index=True로 설정하게되면 보기 불편하게 되어있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['Sex','Survived']].groupby(['Sex'],as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in full_data:\n",
    "    age_avg = dataset['Age'].mean()\n",
    "    age_std = dataset['Age'].std()\n",
    "    age_null_count = dataset['Age'].isnull().sum()\n",
    "    \n",
    "    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n",
    "    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n",
    "    dataset['Age'] = dataset['Age'].astype(int)\n",
    "    \n",
    "train['CategoricalAge'] = pd.cut(train['Age'], 5)\n",
    "\n",
    "print (train[['CategoricalAge', 'Survived']].groupby(['CategoricalAge'], as_index=False).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['SibSp','Survived']].groupby(['SibSp'],as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['Parch','Survived']].groupby(['Parch'],as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in full_data:\n",
    "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
    "train[['FamilySize','Survived']].groupby(['FamilySize'],as_index=False).mean()\n",
    "# FamilySize라는 Feature가 언뜻보면 생존에 영향을 끼치는 것 같기도 하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in full_data:\n",
    "    dataset['IsAlone'] = 0\n",
    "    dataset.loc[dataset['FamilySize']==1,'IsAlone']=1\n",
    "train[['IsAlone','Survived']].groupby(['IsAlone'],as_index=False).mean()\n",
    "# 혼자 배에 탔는지 안탔는지가 영향을 주는 것 같아 보인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어렵게 생각하지않고 Missing Value를 최빈값으로 채워 넣어야겠다.\n",
    "for dataset in full_data:\n",
    "    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n",
    "train[['Embarked','Survived']].groupby(['Embarked'],as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "g = sns.distplot(df_train['Fare'], color='b', \n",
    "                 label='Skewness : {:.2f}'.format(df_train['Fare'].skew()), ax=ax)\n",
    "g = g.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['CategoricalFare'] = pd.qcut(train['Fare'], 4)\n",
    "# pd.qcut -> 동일한 개수로 나누기\n",
    "train[['CategoricalFare','Survived']].groupby(['CategoricalFare'],as_index=False).mean()\n",
    "# 높은 운임을 낸 사람이 조금 더 살아남은것으로 보임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Name']\n",
    "# 보면 이름이 중구난방으로 적혀있어...어떻게할까? 보면 Mr,Miss,Master,Mrs,Rev등으로 나누어짐\n",
    "# 나는 외국이름을 나누는데 어떻게 나눠야할지 모르겠어.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(name):\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name) or re.search(' ([A-Za-z]+).', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "for dataset in full_data:\n",
    "    dataset['Title'] = dataset['Name'].apply(get_title)\n",
    "\n",
    "print(pd.crosstab(train['Title'], train['Sex']))\n",
    "\n",
    "\n",
    "# 이름중에서 성만 따로 추출해서 'Title'이라는 새로운 Column을 만듬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in full_data:\n",
    "    dataset['Title'] = dataset['Title'].replace(['Lady','Capt','Col','Countess','Don','Dr','Jonkheer','Major',\n",
    "                                                'Rev','Sir','Dona'],'Rare')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mlle','Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mme','Mrs')\n",
    "    dataset['Title'] = dataset['Title'].replace('Ms','Miss')\n",
    "    \n",
    "train[['Title','Survived']].groupby(['Title'],as_index=False).mean()\n",
    "\n",
    "# 사람이 타이핑 하면서 오타가 난것들을 바꾸어주고 흔하지 않은 성을 Rare로 하나로 묶어 Survived와의 관계를 확인 하였다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for dataset in full_data:\n",
    "    # 'Sex' : 여자를 1 남자를 0으로 매핑하자\n",
    "    dataset['Sex'] = dataset['Sex'].replace('female',0)\n",
    "    dataset['Sex'] = dataset['Sex'].replace('male',1)\n",
    "    # 'Title' : 5개로 나누어진 'Title'변수를 숫자로 매핑함\n",
    "    dataset['Title'] = dataset['Title'].replace('Master',1)\n",
    "    dataset['Title'] = dataset['Title'].replace('Miss',2)\n",
    "    dataset['Title'] = dataset['Title'].replace('Mr',3)\n",
    "    dataset['Title'] = dataset['Title'].replace('Mrs',4)\n",
    "    dataset['Title'] = dataset['Title'].replace('Rare',5)\n",
    "\n",
    "    # 'Embarked' C,Q,S로 나누어져있음 이걸 숫자로 매핑하자\n",
    "    dataset['Embarked'] = dataset['Embarked'].replace('C',0)\n",
    "    dataset['Embarked'] = dataset['Embarked'].replace('Q',1)\n",
    "    dataset['Embarked'] = dataset['Embarked'].replace('S',2)\n",
    "    # 'Fare' 구간을 숫자로 매핑하자\n",
    "    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n",
    "    dataset.loc[ (dataset['Fare'] > 7.91) & (dataset['Fare']< 14.454), 'Fare'] = 1\n",
    "    dataset.loc[ (dataset['Fare'] > 14.454) & (dataset['Fare']< 31.0), 'Fare'] = 2\n",
    "    dataset.loc[ dataset['Fare'] > 31.0, 'Fare'] = 3\n",
    "    # 'Age' 구간을 숫자로 매핑하자\n",
    "    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n",
    "    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n",
    "    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n",
    "    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n",
    "    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4\n",
    "# Feature들중에서 필요없는건 지우고 필요한건 선택하여 train set을 완성하자.\n",
    "drop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp',\n",
    "                 'Parch', 'FamilySize']\n",
    "train = train.drop(drop_elements, axis = 1)\n",
    "train = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\n",
    "\n",
    "test  = test.drop(drop_elements, axis = 1)\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=train['Survived']\n",
    "X=train.drop(columns=['Survived'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.fillna(test.mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(test) \n",
    "rf.score(X_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "# 랜덤포레스트에서 사용하는 parameter들 확인.\n",
    "pprint(rf.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [int(x) for x in np.linspace(start=200,stop=2000,num=10)]\n",
    "max_features = ['auto', 'sqrt', 'log2']\n",
    "max_depth = [int(x) for x in np.linspace(10,110,num=11)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_grid = { \n",
    "    'n_estimators': n_estimators,\n",
    "    'max_features': max_features,\n",
    "    'max_depth' : max_depth,\n",
    "    'min_samples_split': min_samples_split,\n",
    "    'min_samples_leaf': min_samples_leaf,\n",
    "    'bootstrap': bootstrap}\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions=random_grid,n_iter=100,\n",
    "                              cv=3,verbose=2,random_state=42,\n",
    "                              n_jobs=-1)\n",
    "rf_random.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'bootstrap': [True,False],\n",
    "    'max_depth': [20,30,40],\n",
    "    'max_features': ['sqrt'],\n",
    "    'min_samples_leaf': [2],\n",
    "    'min_samples_split': [2],\n",
    "    'n_estimators': [800,900,1000,1100]\n",
    "}\n",
    "rf = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(estimator = rf,param_grid = param_grid,\n",
    "                          cv=5,n_jobs=-1,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_grid = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_grid.fit(X_train,y_train)\n",
    "best_grid.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2 = pd.read_csv('C:/Users/sangkyu/git/kaggle/titanic/input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pred=best_grid.predict(test)\n",
    "rf_X_pred=best_grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, rf_X_pred)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "max_depths = np.linspace(1, 32, 32, endpoint=True)\n",
    "train_results = []\n",
    "test_results = []\n",
    "for max_depth in max_depths:\n",
    "   dt = RandomForestClassifier(max_depth=max_depth)\n",
    "   dt.fit(X_train, y_train)\n",
    "   train_pred = dt.predict(X_train)\n",
    "   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n",
    "   roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "   # Add auc score to previous train results\n",
    "   train_results.append(roc_auc)\n",
    "   y_pred = dt.predict(X_test)\n",
    "   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "   roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "   # Add auc score to previous test results\n",
    "   test_results.append(roc_auc)\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "line1, = plt.plot(max_depths, train_results, 'b', label=\"Train AUC\")\n",
    "line2, = plt.plot(max_depths, test_results, 'r', label=\"Test AUC\")\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.ylabel('AUC score')\n",
    "plt.xlabel('Tree depth')\n",
    "plt.show()\n",
    "# train셋에 overfitting된걸 볼 수 있음.\n",
    "# overffiting을 방지하기 위해선 적은수의 parameter를 사용해야한다\n",
    "# 또한 모델의 hyperparameter들을 바꿔서 줄일 수 있다\n",
    "# regularization 하면된다\n",
    "# 더많은 training data를 사용하면된다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(test_2['PassengerId'])\n",
    "result['Survived']= rf_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('submission13.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
